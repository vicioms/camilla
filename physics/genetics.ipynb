{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import os\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ae3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = np.load('covariances.npz', allow_pickle=True)\n",
    "dict_cov_stem = saved_data['dict_cov_stem'].item()\n",
    "dict_cov_prog = saved_data['dict_cov_prog'].item()\n",
    "dict_cov_diff = saved_data['dict_cov_diff'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_W_to_PD(W, D, margin=1e-3):\n",
    "    # D is diagonal (array or diag matrix)\n",
    "    d = np.diag(D) if D.ndim == 2 else np.asarray(D)\n",
    "    Dm12 = 1.0/np.sqrt(d)\n",
    "    # compute spectral norm of D^{-1/2} W^T\n",
    "    Z = (W.T * Dm12[:,None])   # columns scaled by D^{-1/2}\n",
    "    # spectral norm via largest svd value\n",
    "    smax = np.linalg.svd(Z, compute_uv=False)[0]\n",
    "    cap = 1.0 - margin\n",
    "    if smax > cap:\n",
    "        W *= cap / smax\n",
    "    return W\n",
    "\n",
    "def fit_W_l1_l2(\n",
    "    X_cov,\n",
    "    inverse_variances,\n",
    "    w_diag_val,\n",
    "    l1_reg_strength=1e-2,\n",
    "    l2_reg_strength=1e-2,\n",
    "    n_iters=10000,\n",
    "    learning_rate=0.001,\n",
    "    tolerance=1e-16,          # for eigvals, as before\n",
    "    early_stopping=True,\n",
    "    es_patience=500,          # number of steps with no improvement\n",
    "    es_min_delta=1e-6         # minimum improvement to reset patience\n",
    "):\n",
    "    vals, vecs = np.linalg.eigh(X_cov)\n",
    "    vals = np.maximum(vals, tolerance)\n",
    "    X_fit = torch.tensor((vecs * vals) @ vecs.T, dtype=torch.float32)\n",
    "    if(inverse_variances is float):\n",
    "        fixed_X_matr = X_fit * inverse_variances - torch.eye(X_fit.size(0))\n",
    "    else:\n",
    "        fixed_X_matr = X_fit @ torch.diag(torch.Tensor(inverse_variances)) - torch.eye(X_fit.size(0))\n",
    "\n",
    "    W = torch.randn(size=(X_cov.shape[0], X_cov.shape[0]))/np.sqrt(X_cov.shape[0])\n",
    "    W = W.requires_grad_()\n",
    "    optimizer = optim.Adam([W], lr=learning_rate, weight_decay=l2_reg_strength)\n",
    "\n",
    "    indices = torch.arange(W.shape[0])\n",
    "    loss_history = []\n",
    "    std_history = []\n",
    "\n",
    "    best_loss = None\n",
    "    steps_no_improve = 0\n",
    "\n",
    "    for step in trange(n_iters):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        WTW = W.T @ W\n",
    "        loss = torch.norm(fixed_X_matr - WTW @ X_fit, p='fro')**2\n",
    "\n",
    "        if l1_reg_strength > 0:\n",
    "            loss += l1_reg_strength * torch.norm(W, p=1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = float(loss.item())\n",
    "        loss_history.append(current_loss)\n",
    "\n",
    "        # --- early stopping bookkeeping ---\n",
    "        if early_stopping:\n",
    "            if best_loss is None or (best_loss - current_loss) > es_min_delta:\n",
    "                best_loss = current_loss\n",
    "                steps_no_improve = 0\n",
    "            else:\n",
    "                steps_no_improve += 1\n",
    "                if steps_no_improve >= es_patience:\n",
    "                    # Optional: uncomment if you want a message\n",
    "                    # print(f\"Early stopping at step {step+1}, best loss = {best_loss:.6g}\")\n",
    "                    break\n",
    "\n",
    "        # --- diagnostics on W (as you had) ---\n",
    "        with torch.no_grad():\n",
    "\n",
    "            if(w_diag_val is not None):\n",
    "                W[indices, indices] = w_diag_val\n",
    "            W_np = W.detach().cpu().numpy()\n",
    "            W_upper = W_np[np.triu_indices(W_np.shape[0], k=1)]\n",
    "            W_lower = W_np[np.tril_indices(W_np.shape[0], k=-1)]\n",
    "            std_history.append(np.concatenate((W_upper, W_lower)).std())\n",
    "\n",
    "    loss_history = np.array(loss_history)\n",
    "    std_history = np.array(std_history)\n",
    "    return W.detach().cpu().numpy(), loss_history, std_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_W_bhatt(\n",
    "    X_cov,\n",
    "    inverse_variances,\n",
    "    w_diag_val,\n",
    "    n_iters=10000,\n",
    "    learning_rate=0.001,\n",
    "    tolerance=1e-16,          # for eigvals, as before\n",
    "    early_stopping=True,\n",
    "    es_patience=500,          # number of steps with no improvement\n",
    "    es_min_delta=1e-6         # minimum improvement to reset patience\n",
    "):\n",
    "    vals, vecs = np.linalg.eigh(X_cov)\n",
    "    vals = np.maximum(vals, tolerance)\n",
    "    X_fit = (vecs * vals) @ vecs.T\n",
    "\n",
    "    W = np.random.randn(X_cov.shape[0], X_cov.shape[0])/np.sqrt(X_cov.shape[0])\n",
    "    G = np.eye(X_cov.shape[0])*inverse_variances\n",
    "\n",
    "    indices = np.arange(W.shape[0])\n",
    "    loss_history = []\n",
    "    std_history = []\n",
    "\n",
    "    best_loss = None\n",
    "    steps_no_improve = 0\n",
    "\n",
    "    for step in trange(n_iters):\n",
    "\n",
    "        WTW = W.T @ W\n",
    "        Y = G - WTW\n",
    "\n",
    "        id_plus_C_Y = np.eye(X_cov.shape[0]) + np.matmul(X_fit, Y)\n",
    "\n",
    "        loss = 0.5*np.log(np.linalg.det(id_plus_C_Y)/np.linalg.det(Y)**0.5)\n",
    "\n",
    "        W += learning_rate * W @ (np.linalg.inv(id_plus_C_Y) @ X_fit -0.5*np.linalg.inv(Y))\n",
    "\n",
    "        project_W_to_PD(W, G)\n",
    "        current_loss = float(loss.item())\n",
    "        loss_history.append(current_loss)\n",
    "\n",
    "        # --- early stopping bookkeeping ---\n",
    "        if early_stopping:\n",
    "            if best_loss is None or (best_loss - current_loss) > es_min_delta:\n",
    "                best_loss = current_loss\n",
    "                steps_no_improve = 0\n",
    "            else:\n",
    "                steps_no_improve += 1\n",
    "                if steps_no_improve >= es_patience:\n",
    "                    # Optional: uncomment if you want a message\n",
    "                    # print(f\"Early stopping at step {step+1}, best loss = {best_loss:.6g}\")\n",
    "                    break\n",
    "\n",
    "        # --- diagnostics on W (as you had) ---\n",
    "        if(w_diag_val is not None):\n",
    "            W[indices, indices] = w_diag_val\n",
    "        W_upper = W[np.triu_indices(W.shape[0], k=1)]\n",
    "        W_lower = W[np.tril_indices(W.shape[0], k=-1)]\n",
    "        std_history.append(np.concatenate((W_upper, W_lower)).std())\n",
    "\n",
    "    loss_history = np.array(loss_history)\n",
    "    std_history = np.array(std_history)\n",
    "    return W, loss_history, std_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57577cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_per_type = {}\n",
    "W_per_type = {}\n",
    "for tp, dict_cov in [('stem', dict_cov_stem), ('prog', dict_cov_prog), ('diff', dict_cov_diff)]:\n",
    "    std_per_type[tp] = {}\n",
    "    W_per_type[tp] = {}\n",
    "    for header in dict_cov.keys():\n",
    "        X_cov = dict_cov[header]['Cov']\n",
    "        X_cov_to_fit = X_cov/(np.outer(np.diag(X_cov)**0.5, np.diag(X_cov)**0.5))\n",
    "        max_ev = np.max(np.linalg.eigvalsh(X_cov_to_fit))\n",
    "        init_inv_variances = 1/np.diag(X_cov_to_fit)\n",
    "        #W, loss_history, std_history = fit_W_l1_l2(X_cov, init_inv_variances, 1/np.sqrt(X_cov.shape[0]), l1_reg_strength=0.0, l2_reg_strength=0.0, n_iters=20000, learning_rate=0.001)\n",
    "        W, loss_history, std_history = fit_W_l1_l2(X_cov_to_fit,\n",
    "                                                   init_inv_variances,\n",
    "                                                   w_diag_val= None, #1/np.sqrt(X_cov.shape[0]),\n",
    "                                                    #l1_reg_strength=0,\n",
    "                                                    #l2_reg_strength=0,\n",
    "                                                    n_iters=5000,\n",
    "                                                    learning_rate=0.001,\n",
    "                                                    tolerance=1e-16,          # for eigvals, as before\n",
    "                                                    early_stopping=False,\n",
    "                                                    es_patience=3000,          # number of steps with no improvement\n",
    "                                                    es_min_delta=1e-6         # minimum improvement to reset patience\n",
    "                                                    )\n",
    "        #W, loss_history, std_history = fit_W_bhatt(X_cov,\n",
    "        #                                           init_inv_variances,\n",
    "        #                                           w_diag_val= 1/np.sqrt(X_cov.shape[0]),\n",
    "        #                                            n_iters=50000,\n",
    "        #                                            learning_rate=0.001,\n",
    "        #                                            tolerance=1e-16,          # for eigvals, as before\n",
    "        #                                            early_stopping=False,\n",
    "        #                                            es_patience=1000,          # number of steps with no improvement\n",
    "        #                                            es_min_delta=1e-6          # minimum improvement to reset patience\n",
    "        #                                            )\n",
    "        \n",
    "        W_per_type[tp][header] = (W, init_inv_variances, X_cov_to_fit)\n",
    "        W_upper = W[np.triu_indices(W.shape[0], k=1)]\n",
    "        W_lower = W[np.tril_indices(W.shape[0], k=-1)]\n",
    "        std_per_type[tp][header] = {'std' : np.concatenate((W_upper, W_lower)).std(), 'max_ev' : max_ev}\n",
    "        std = std_per_type[tp][header]['std']\n",
    "        fig, axs = plt.subplots(1,3, figsize=(10,4))\n",
    "        axs[0].plot(std_history, label=header)\n",
    "        axs[1].plot(loss_history, label=header)\n",
    "        axs[2].imshow(W, vmin=-0.5, vmax=0.5, cmap='seismic')\n",
    "        axs[0].set_title(f'History for {header}')\n",
    "        axs[2].set_title(f'Std: {std:.3g}, max_ev: {max_ev:.3g}')\n",
    "        axs[0].set_xlabel('Iteration')\n",
    "        axs[0].set_ylabel('Std')\n",
    "        axs[0].legend()\n",
    "        axs[1].set_yscale('log')\n",
    "        axs[1].set_xscale('log')\n",
    "        axs[1].set_xlabel('Iteration')\n",
    "        axs[1].set_ylabel('Loss')\n",
    "        axs[1].legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "stem_stds = []\n",
    "stem_stds.extend([std_per_type['stem'][header]['std'] for header in std_per_type['stem'].keys()])\n",
    "stem_stds.extend([std_per_type['prog'][header]['std'] for header in std_per_type['prog'].keys()])\n",
    "stem_stds = np.array(stem_stds)\n",
    "diff_stds = np.array([std_per_type['diff'][header]['std'] for header in std_per_type['diff'].keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaler = 0.5/np.mean(np.concatenate((stem_stds, diff_stds)))\n",
    "np.mean(stem_stds)/np.mean(diff_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcc12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mannwhitneyu(stem_stds, diff_stds, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp in W_per_type.keys():\n",
    "    if tp == 'stem':\n",
    "        dict_cov = dict_cov_stem\n",
    "    elif tp == 'prog':\n",
    "        dict_cov = dict_cov_prog\n",
    "    elif tp == 'diff':\n",
    "        dict_cov = dict_cov_diff\n",
    "        \n",
    "    for header in dict_cov.keys():\n",
    "        W,init_inv_variances,X_cov_to_fit = W_per_type[tp][header]\n",
    "        X_fit = np.linalg.inv(np.eye(X_cov_to_fit.shape[0])*init_inv_variances - W.T @ W)\n",
    "        print(np.linalg.eigvalsh(X_fit))\n",
    "        #plt.hist()\n",
    "        #plt.hist(np.linalg.eigvalsh(X_fit), alpha=0.7, bins=10)\n",
    "        #plt.yscale('log')\n",
    "        #plt.show()\n",
    "        fig, axs = plt.subplots(1,3, figsize=(14,4))\n",
    "        axs[0].imshow(X_cov_to_fit, vmin=-1.0, vmax=1.0, cmap='seismic')\n",
    "        axs[1].imshow(X_fit, vmin=-1.0, vmax=1.0, cmap='seismic')\n",
    "        axs[2].hist(np.linalg.eigvalsh(X_cov_to_fit), bins=10, label='Original')\n",
    "        axs[2].hist(np.linalg.eigvalsh(X_fit), alpha=0.7, bins=10, label='Fitted')\n",
    "        axs[2].legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
