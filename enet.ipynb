{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bfb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from math import sqrt, log\n",
    "import math\n",
    "from typing import Sequence, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional VAE for square images with pixels in [0,1].\n",
    "    Uses Bernoulli decoder with BCE-with-logits reconstruction loss.\n",
    "\n",
    "    - forward(x) returns (x_logits, mu, logvar)\n",
    "    - reconstruct(x) returns sigmoid(x_logits)\n",
    "    - sample(n) returns samples in [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,                 # H = W = input_size\n",
    "        input_channels: int,             # e.g. 1 (MNIST) or 3 (RGB)\n",
    "        hidden_channels: Sequence[int],  # e.g. (32, 64, 128)\n",
    "        latent_dim: int,\n",
    "        act: str = \"relu\",               # \"relu\" or \"silu\" or \"lrelu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # NOTE: no power-of-two constraint. Any positive size works.\n",
    "        if input_size <= 0:\n",
    "            raise ValueError(\"input_size must be positive.\")\n",
    "\n",
    "        if len(hidden_channels) < 1:\n",
    "            raise ValueError(\"hidden_channels must have at least one element.\")\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = list(hidden_channels)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # --- Activation ---\n",
    "        if act == \"relu\":\n",
    "            Act = lambda: nn.ReLU(inplace=True)\n",
    "        elif act == \"silu\":\n",
    "            Act = lambda: nn.SiLU(inplace=True)\n",
    "        elif act == \"lrelu\":\n",
    "            Act = lambda: nn.LeakyReLU(0.2, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown act='{act}'\")\n",
    "\n",
    "        # --- Encoder conv stack ---\n",
    "        enc_layers = []\n",
    "        in_ch = input_channels\n",
    "        for out_ch in self.hidden_channels:\n",
    "            enc_layers += [\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=2, padding=1),\n",
    "                Act(),\n",
    "            ]\n",
    "            in_ch = out_ch\n",
    "        self.encoder = nn.Sequential(*enc_layers)\n",
    "\n",
    "        # Determine encoder output shape dynamically (robust)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, input_channels, input_size, input_size)\n",
    "            h = self.encoder(dummy)\n",
    "            self._enc_shape = tuple(h.shape[1:])  # (C', H', W')\n",
    "            self._enc_flat_dim = h.flatten(1).shape[1]\n",
    "\n",
    "        # Latent heads\n",
    "        self.fc_mu = nn.Linear(self._enc_flat_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self._enc_flat_dim, latent_dim)\n",
    "\n",
    "        # --- Decoder head: latent -> conv feature map ---\n",
    "        self.fc_dec = nn.Linear(latent_dim, self._enc_flat_dim)\n",
    "\n",
    "        # --- Decoder conv-transpose stack ---\n",
    "        # mirror hidden_channels in reverse\n",
    "        dec_layers = []\n",
    "        rev = list(reversed(self.hidden_channels))\n",
    "        for i, in_ch in enumerate(rev):\n",
    "            out_ch = input_channels if i == (len(rev) - 1) else rev[i + 1]\n",
    "            dec_layers.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_ch, out_ch,\n",
    "                    kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "                )\n",
    "            )\n",
    "            # no activation on last layer: output are logits\n",
    "            if i != (len(rev) - 1):\n",
    "                dec_layers.append(Act())\n",
    "        self.decoder = nn.Sequential(*dec_layers)\n",
    "\n",
    "        # Optional: warn if shape mismatch happens (odd sizes + many downsamples can do this)\n",
    "        with torch.no_grad():\n",
    "            z0 = torch.zeros(1, latent_dim)\n",
    "            x_logits0 = self.decode_logits(z0)\n",
    "            if x_logits0.shape[-2:] != (input_size, input_size):\n",
    "                # Don't hard-fail; just make it obvious early.\n",
    "                raise ValueError(\n",
    "                    f\"Decoder output spatial size {tuple(x_logits0.shape[-2:])} \"\n",
    "                    f\"does not match input_size {(input_size, input_size)}. \"\n",
    "                    f\"Fix by adjusting num layers/strides or decoder output_padding.\"\n",
    "                )\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        h = self.encoder(x).flatten(1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode_logits(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.fc_dec(z).view(z.size(0), *self._enc_shape)\n",
    "        x_logits = self.decoder(h)\n",
    "        return x_logits\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_logits = self.decode_logits(z)\n",
    "        return x_logits, mu, logvar\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_logits, _, _ = self.forward(x)\n",
    "        return torch.sigmoid(x_logits)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n: int, device: torch.device | None = None) -> torch.Tensor:\n",
    "        if device is None:\n",
    "            device = next(self.parameters()).device\n",
    "        z = torch.randn(n, self.latent_dim, device=device)\n",
    "        x_logits = self.decode_logits(z)\n",
    "        return torch.sigmoid(x_logits)\n",
    "\n",
    "def vae_loss_bce_logits(\n",
    "    x: torch.Tensor,\n",
    "    x_logits: torch.Tensor,\n",
    "    mu: torch.Tensor,\n",
    "    logvar: torch.Tensor,\n",
    "    beta: float = 1.0,\n",
    "    reduction: str = \"sum\",\n",
    ") -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    ELBO = recon + beta * KL\n",
    "    recon: Bernoulli likelihood => BCE-with-logits\n",
    "    KL: q(z|x) || N(0,I)\n",
    "    \"\"\"\n",
    "    if reduction not in (\"sum\", \"mean\"):\n",
    "        raise ValueError(\"reduction must be 'sum' or 'mean'\")\n",
    "\n",
    "    recon = F.binary_cross_entropy_with_logits(x_logits, x, reduction=\"sum\")\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    loss = recon + beta * kl\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss / x.size(0)\n",
    "        recon = recon / x.size(0)\n",
    "        kl = kl / x.size(0)\n",
    "\n",
    "    stats = {\"loss\": loss.detach(), \"recon\": recon.detach(), \"kl\": kl.detach()}\n",
    "    return loss, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a87244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    beta: float = 1.0,\n",
    "    reduction: str = \"mean\",\n",
    ") -> Dict[str, float]:\n",
    "    model.train()\n",
    "    totals = {\"loss\": 0.0, \"recon\": 0.0, \"kl\": 0.0}\n",
    "    n_batches = 0\n",
    "\n",
    "    for x, *_ in dataloader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        x_logits, mu, logvar = model(x)\n",
    "        loss, stats = vae_loss_bce_logits(x, x_logits, mu, logvar, beta=beta, reduction=reduction)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for k in totals:\n",
    "            totals[k] += float(stats[k])\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in totals:\n",
    "        totals[k] /= max(1, n_batches)\n",
    "    return totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "input_channels = 1\n",
    "hidden_channels = (32, 64)\n",
    "latent_dim = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"mps\"\n",
    "model = ConvVAE(\n",
    "    input_size=input_size,\n",
    "    input_channels=input_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    latent_dim=latent_dim,\n",
    "    act=\"relu\",\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f74da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # ← convert RGB → grayscale\n",
    "    transforms.ToTensor(),                         # shape: (1, 32, 32)\n",
    "])\n",
    "dataset = torchvision.datasets.CIFAR10(root=\"./sample_data\", train=True, download=False, transform=transform)\n",
    "dataset = torchvision.datasets.MNIST(root=\"./sample_data\", train=True, download=False, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./sample_data\", train=False, download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7399ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    stats = train_one_epoch(model, dataloader, optimizer, device, beta=1.0, reduction=\"mean\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {stats['loss']:.4f}, Recon: {stats['recon']:.4f}, KL: {stats['kl']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a80ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "x, y = next(iter(test_dataloader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon = model.reconstruct(x.to(device)).to(\"cpu\")\n",
    "    mean, logvariance  = model.encode(x.to(device))\n",
    "    mean = mean.to(\"cpu\")\n",
    "    logvariance = logvariance.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c55dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = mean + torch.randn_like(logvariance)*torch.exp(0.5 * logvariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706df2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_matrix =  torch.einsum('ki,li->kl', patterns.view(patterns.size(0), -1), patterns.view(patterns.size(0), -1))\n",
    "diag = torch.diagonal(gram_matrix)\n",
    "C = gram_matrix / torch.sqrt(torch.outer(diag, diag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3cc88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Step 1000\n",
      "Step 2000\n",
      "Step 3000\n",
      "Step 4000\n",
      "Step 5000\n",
      "Step 6000\n",
      "Step 7000\n",
      "Step 8000\n",
      "Step 9000\n",
      "Step 10000\n",
      "Step 11000\n",
      "Step 12000\n",
      "Step 13000\n",
      "Step 14000\n",
      "Step 15000\n",
      "Step 16000\n",
      "Step 17000\n",
      "Step 18000\n",
      "Step 19000\n",
      "Step 20000\n",
      "Step 21000\n",
      "Step 22000\n"
     ]
    }
   ],
   "source": [
    "# dual approach\n",
    "betas = torch.logspace(0, 2, 100)\n",
    "K = C.size(0)\n",
    "w = torch.distributions.dirichlet.Dirichlet(torch.ones(K)).sample((betas.size(0),))\n",
    "eta = 0.9\n",
    "# w = softmax(beta*C @ w)\n",
    "for step in range(40000):\n",
    "    current = torch.softmax(betas[:,None] * w @ C, dim=1)\n",
    "    w = eta*w + (1-eta)*current\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}\")\n",
    "    #w += dt*(w @ C - (torch.log(w)+1)/betas[:, None])\n",
    "    #w /= w.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f597b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_patterns = torch.sum(w[:, :, None]*patterns[None, :, :], dim=1)\n",
    "with torch.no_grad():\n",
    "    recon_patterns = torch.sigmoid(model.decode_logits(recovered_patterns.to(device))).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8553257a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(figsize=(4, 4))\n",
    "ims = []\n",
    "for i in range(betas.size(0)):\n",
    "    im = axes.imshow(\n",
    "        recon_patterns[i, 0],\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cmap=\"gray\",\n",
    "        animated=True\n",
    "    )\n",
    "    ims.append([im])\n",
    "axes.axis(\"off\")\n",
    "ani = animation.ArtistAnimation(\n",
    "    fig,\n",
    "    ims,\n",
    "    interval=50,   # ms between frames\n",
    "    blit=True)\n",
    "ani.save(\"recon_animation.mp4\", fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(betas, -(torch.log(w)*w).sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "868402f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = torch.zeros((betas.size(0), 10), device=w.device, dtype=w.dtype)\n",
    "\n",
    "class_weight = class_weight.scatter_add_(\n",
    "    dim=1,\n",
    "    index=y.unsqueeze(0).expand(betas.size(0), K),\n",
    "    src=w\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def43fa2",
   "metadata": {},
   "source": [
    "### Analytical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1024\n",
    "B = K // 32\n",
    "rho1 = 1.0/(B-1)\n",
    "rho0 = 0.99/(K-1)\n",
    "m_a = torch.arange(1,1+B).float()\n",
    "m_b = torch.arange(1, 1 + K//B).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for beta in torch.logspace(0, 5, 25):\n",
    "    F = (1-rho0)/(2*m_a[:,None]*m_b[None,:]) + torch.log(m_a[:,None]*m_b[None,:])/(beta) + (rho0-rho1)/(2*m_b)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.plot_surface(*torch.meshgrid(m_a, m_b), F, facecolors=plt.cm.viridis((F - F.min()) / (F.max() - F.min())), rstride=1, cstride=1, antialiased=False)\n",
    "    ax.set_xlabel(\"m_a\")\n",
    "    ax.set_ylabel(\"m_b\")\n",
    "    ax.set_title( f\"Beta: {beta:.2e} \" + str(1+torch.tensor(torch.unravel_index(torch.argmax(F), F.shape))))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1024\n",
    "B = K // 32\n",
    "N = 16\n",
    "rho1 = 0.1\n",
    "rho0 = -0.5/(K-1)\n",
    "C = rho0*torch.ones((K,K))\n",
    "for g in range(0,K, B):\n",
    "    C[g:g+B, g:g+B] = rho1  \n",
    "#rho = 0.1/(K-1)\n",
    "#C = rho*torch.ones((K,K))\n",
    "_ = torch.diagonal(C).fill_(1.0)\n",
    "L = torch.linalg.cholesky(C) \n",
    "P = (torch.randn((N,K)) @ L.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = torch.arange(1, 1+K).float()\n",
    "#F = (1-rho)/(2*m[None,:]) + torch.log(m[None,:])/(betas[:,None])\n",
    "#plt.contourf(torch.log(F), levels=20)\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b95ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dual approach\n",
    "betas = torch.logspace(-5, 5, 200)\n",
    "w = torch.distributions.dirichlet.Dirichlet(torch.ones(K)).sample((betas.size(0),))\n",
    "eta = 0.9\n",
    "# w = softmax(beta*C @ w)\n",
    "for step in range(5000):\n",
    "    current = torch.softmax(betas[:,None] * w @ C, dim=1)\n",
    "    w = eta*w + (1-eta)*current\n",
    "    #w += dt*(w @ C - (torch.log(w)+1)/betas[:, None])\n",
    "    #w /= w.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(betas, - (torch.log(w)*w).sum(dim=1)/log(K))\n",
    "#plt.gca().twinx().plot(betas,torch.clip(betas*(1-rho)/2, 1, K).int()/K)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\\\beta$')\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = torch.linspace(0.5, 1.0, 200)\n",
    "x = torch.randn((betas.size(0), N))\n",
    "dt =0.01\n",
    "for step in range(100000):\n",
    "    y = (betas[:, None, None] * P[None, :, :] * x[:, None, :]).sum(dim=2)\n",
    "    w = torch.softmax(y, dim=-1)\n",
    "    x += dt*((P[None, :, :]*w[:, :, None]).sum(dim=1) - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c661154",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(betas, -torch.sum(w*torch.log(w), dim=1) / torch.log(torch.tensor(K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf88aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(betas, torch.sum(w**4, dim=1)/torch.sum(w**2, dim=1)**2, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd9e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
