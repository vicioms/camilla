{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce290b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sequences import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a4ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll_loss(log_prob, log_surv, seq_lengths):\n",
    "    mask = torch.arange(log_prob.size(1))[None, :] < seq_lengths[:, None]\n",
    "    nll_loss = - (log_prob * mask).sum(-1)\n",
    "    log_surv_last = torch.gather(log_surv, dim=-1, index=torch.unsqueeze(seq_lengths,-1))\n",
    "    return nll_loss - log_surv_last.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b3e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gam_filter(values, min_val, max_val):\n",
    "    rescaled_values = 2*(values - min_val) / (max_val - min_val) - 1.0\n",
    "    phi_values = np.arccos(rescaled_values) \n",
    "    return np.sin(phi_values[:,None] - phi_values[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed3037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_origin = pd.to_datetime(\"1997-01-01T00:00:00\").to_numpy()\n",
    "catalog = pd.read_csv(\"japanese-cat.csv\", sep=\" \", parse_dates=[\"Origin_Time(UT)\"])\n",
    "times = catalog[\"Origin_Time(UT)\"].to_numpy()\n",
    "times_days = (times - time_origin) / np.timedelta64(1, 'D')\n",
    "times = times_days.astype(np.float32)\n",
    "magnitudes = catalog[\"JMA_Magnitude(Mj)\"].to_numpy()\n",
    "latitudes = catalog[\"Latitude(deg)\"].to_numpy()\n",
    "longitudes = catalog[\"Longitude(deg)\"].to_numpy()\n",
    "features = np.vstack([magnitudes, latitudes, longitudes]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0081bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequence_arrival_times, subsequence_features, t_start, t_end = extract_subsequences(\n",
    "    times,\n",
    "    features,\n",
    "    max_num_sequences = 100,\n",
    "    duration_scale = 30\n",
    ")\n",
    "subsequence_waiting_times = [ arrival_to_inter_times(subsequence_arrival_times[i], t_start[i], t_end[i])  for i in range(len(subsequence_arrival_times))]\n",
    "subsequence_list = [ np.vstack((subsequence_waiting_times[i],np.vstack([subsequence_features[i], np.zeros(subsequence_features[i].shape[1])]).T)).T for i in range(len(subsequence_features))]\n",
    "\n",
    "packed_features, seq_lengths, masks = pack_sequences(\n",
    "    subsequence_list, batch_first=True, valid_mask_is_true=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90027c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaitingTimeModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_params):\n",
    "        super(WaitingTimeModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_params = num_params\n",
    "\n",
    "    def forward(self, x, tau):\n",
    "        return None\n",
    "        \n",
    "class WeibullWaitingTimeModel(WaitingTimeModel):\n",
    "    def __init__(self, input_dim):\n",
    "        super(WeibullWaitingTimeModel, self).__init__(input_dim, num_params=2)\n",
    "        self.linear = nn.Linear(input_dim, 2)\n",
    "\n",
    "    def forward(self, x, tau):\n",
    "        params = self.linear(x)\n",
    "        scale = torch.log(1+torch.exp(params[..., 0]))\n",
    "        shape = torch.log(1+torch.exp(params[..., 1]))\n",
    "        scaled_taus = tau/scale\n",
    "        log_prob = torch.log(shape) - torch.log(scale) + (shape - 1)*torch.log(scaled_taus) - scaled_taus**shape\n",
    "        log_surv = - scaled_taus**shape\n",
    "        return log_prob, log_surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82459dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEncoder(torch.nn.Module):\n",
    "    def __init__(self, feature_dim: int, emb_dim: int, nhead: int, dim_feedforward: int, num_layers: int):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        self.emb_layer = torch.nn.Linear(feature_dim, emb_dim)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_key_padding_mask: torch.Tensor = None):\n",
    "        src = self.emb_layer(src)\n",
    "        casual_mask = torch.nn.Transformer.generate_square_subsequent_mask(src.shape[1], device=src.device)\n",
    "        output = self.transformer_encoder(src, is_causal=True, mask=casual_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SequenceEncoder(feature_dim=4, emb_dim=16, nhead=2, dim_feedforward=64, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd61da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = encoder(torch.tensor(packed_features, dtype=torch.float32), \n",
    "                 torch.tensor(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtmodel = WeibullWaitingTimeModel(input_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob, log_surv = wtmodel(output, torch.tensor(packed_features, dtype=torch.float32)[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e63629",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nll_loss(log_prob, log_surv, torch.from_numpy(seq_lengths) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
