{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce290b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sequences import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll_loss(log_prob, log_surv, seq_lengths):\n",
    "    mask = torch.arange(log_prob.size(1))[None, :] < seq_lengths[:, None]\n",
    "    nll_loss = - (log_prob * mask).sum(-1)\n",
    "    log_surv_last = torch.gather(log_surv, dim=-1, index=torch.unsqueeze(seq_lengths,-1))\n",
    "    return nll_loss - log_surv_last.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af1ee0",
   "metadata": {},
   "source": [
    "### What happens\n",
    "$$ z_1 = (\\tau_1, x_1), \\dots, z_n = (\\tau_n, x_n)$$\n",
    "$$ h_i = \\textrm{emb}(z_i) $$\n",
    "$$ h_0 = 0 $$\n",
    "$$ \\tau_c = t_{\\rm start} - t_n$$\n",
    "$$ \\theta_i = \\textrm{net}(h_0, \\dots, h_{i-1}) $$\n",
    "$$ L = -\\sum_{i=1}^n \\ln f(\\tau_i|\\theta_i) - \\ln S(\\tau_c|\\theta_{n+1}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2399a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "\n",
    "class WaitingTimeModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_params):\n",
    "        super(WaitingTimeModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_params = num_params\n",
    "\n",
    "    def forward(self, x, tau):\n",
    "        return None\n",
    "        \n",
    "class WeibullWaitingTimeModel(WaitingTimeModel):\n",
    "    def __init__(self, input_dim):\n",
    "        super(WeibullWaitingTimeModel, self).__init__(input_dim, num_params=2)\n",
    "        self.linear = nn.Linear(input_dim, 2)\n",
    "\n",
    "    def forward(self, x, tau, tau_cens, sequence_lengths):\n",
    "        params = self.linear(x)\n",
    "        raw_scale = F.softplus(params[..., 0])\n",
    "        raw_shape = F.softplus(params[..., 1])\n",
    "        end_idx = torch.unsqueeze(sequence_lengths,-1)\n",
    "        scales_surv = torch.gather(raw_scale, dim=1, index=end_idx)[:,0]\n",
    "        shapes_surv = torch.gather(raw_shape, dim=1, index=end_idx)[:,0]\n",
    "        scales_events = raw_scale[:, :-1]\n",
    "        shapes_events = raw_shape[:, :-1]\n",
    "        tau_clamped = torch.clamp(tau, min=1e-6)\n",
    "        tau_cens_clamped = torch.clamp(tau_cens, min=1e-6)\n",
    "        log_prob = torch.log(shapes_events) - torch.log(scales_events) + (shapes_events - 1)*torch.log(tau_clamped) - (tau_clamped/scales_events)**shapes_events\n",
    "        log_surv = - (tau_cens_clamped/scales_surv)**shapes_surv\n",
    "        return log_prob, log_surv\n",
    "    \n",
    "\n",
    "class SequenceEncoder(torch.nn.Module):\n",
    "    def __init__(self, starting_token_feature_dim : int, feature_dim: int, emb_dim: int, nhead: int, dim_feedforward: int, num_layers: int):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        self.s_token_linear = torch.nn.Linear(starting_token_feature_dim, emb_dim)\n",
    "        self.in_linear = torch.nn.Linear(feature_dim, emb_dim)\n",
    "        self.out_layer = torch.nn.Linear(emb_dim, emb_dim)\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, start_conditioner : torch.Tensor, x: torch.Tensor, sequence_lengths : torch.Tensor):\n",
    "        #starting_token = self.s_token_linear(start_conditioner).unsqueeze(1)\n",
    "        starting_token = torch.zeros((x.size(0), 1, self.s_token_linear.out_features), device=x.device)\n",
    "        output = self.in_linear(x)\n",
    "        output = torch.cat([starting_token, output], dim=1)\n",
    "        casual_mask = torch.nn.Transformer.generate_square_subsequent_mask(output.size(1), device=output.device)\n",
    "        padding_mask = (torch.arange(output.size(1), device=output.device).unsqueeze(0) >= (sequence_lengths+1).unsqueeze(1)).to(torch.float32)\n",
    "        output = self.transformer_encoder(output, is_causal=True, mask=casual_mask, src_key_padding_mask=padding_mask)\n",
    "        output = self.out_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "31b3e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gam_filter(values, min_val, max_val):\n",
    "    rescaled_values = 2*(values - min_val) / (max_val - min_val) - 1.0\n",
    "    phi_values = np.arccos(rescaled_values) \n",
    "    return np.sin(phi_values[:,None] - phi_values[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8ed3037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_origin = pd.to_datetime(\"1997-01-01T00:00:00\").to_numpy()\n",
    "catalog = pd.read_csv(\"japanese-cat.csv\", sep=\" \", parse_dates=[\"Origin_Time(UT)\"])\n",
    "times = catalog[\"Origin_Time(UT)\"].to_numpy()\n",
    "times_days = (times - time_origin) / np.timedelta64(1, 'D')\n",
    "times = times_days.astype(np.float32)\n",
    "magnitudes = catalog[\"JMA_Magnitude(Mj)\"].to_numpy()\n",
    "latitudes = catalog[\"Latitude(deg)\"].to_numpy()\n",
    "longitudes = catalog[\"Longitude(deg)\"].to_numpy()\n",
    "features = np.vstack([magnitudes, latitudes, longitudes]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6dd61da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SequenceEncoder(starting_token_feature_dim=1, feature_dim=4, emb_dim=16, nhead=2, dim_feedforward=64, num_layers=1)\n",
    "wtmodel = WeibullWaitingTimeModel(input_dim=16)\n",
    "sequence = Sequence(arrival_times=times, features=features)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1838d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subseq_inter_times, subseq_cens_inter_times, subseq_features, t_start, t_end = sequence.sample_sequences(\n",
    "    max_num_sequences=100,\n",
    "    duration_scale=30.0,\n",
    "    return_inter_times=True)\n",
    "subseq_features = [ np.vstack([taus, feats.T]).T for taus, feats in zip(subseq_inter_times, subseq_features) ]\n",
    "packed_features, sequence_lengths = Sequence.pack_sequences(\n",
    "    subseq_features)\n",
    "\n",
    "packed_features = torch.tensor(packed_features, dtype=torch.float32)\n",
    "sequence_lengths = torch.tensor(sequence_lengths, dtype=torch.long)\n",
    "t_start = torch.tensor(t_start, dtype=torch.float32)\n",
    "subseq_cens_inter_times = torch.tensor(subseq_cens_inter_times, dtype=torch.float32)\n",
    "output = encoder(\n",
    "    t_start[..., None],   \n",
    "    packed_features, \n",
    "    sequence_lengths)\n",
    "\n",
    "log_prob, log_surv = wtmodel(output, packed_features[...,0],\n",
    "                              subseq_cens_inter_times, sequence_lengths)\n",
    "\n",
    "log_prob_mask = (torch.arange(log_prob.size(1))[None, :] < sequence_lengths[:, None]).to(log_prob.device)\n",
    "\n",
    "loss = - (log_prob * log_prob_mask).sum(-1) - log_surv\n",
    "loss = loss.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
