{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce290b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sequences import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af1ee0",
   "metadata": {},
   "source": [
    "### What happens\n",
    "$$ z_1 = (\\tau_1, x_1), \\dots, z_n = (\\tau_n, x_n)$$\n",
    "$$ h_i = \\textrm{emb}(z_i) $$\n",
    "$$ h_0 = 0 $$\n",
    "$$ \\tau_c = t_{\\rm start} - t_n$$\n",
    "$$ \\theta_i = \\textrm{net}(h_0, \\dots, h_{i-1}) $$\n",
    "$$ L = -\\sum_{i=1}^n \\ln f(\\tau_i|\\theta_i) - \\ln S(\\tau_c|\\theta_{n+1}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2399a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import sigma\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import torch.optim as optim\n",
    "class WaitingTimeModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_params):\n",
    "        super(WaitingTimeModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_params = num_params\n",
    "\n",
    "    def forward(self, x, tau):\n",
    "        return None\n",
    "        \n",
    "class WeibullWaitingTimeModel(WaitingTimeModel):\n",
    "    def __init__(self, input_dim):\n",
    "        super(WeibullWaitingTimeModel, self).__init__(input_dim, num_params=2)\n",
    "        self.linear = nn.Linear(input_dim, 2)\n",
    "\n",
    "    def forward(self, x, tau, tau_cens, sequence_lengths):\n",
    "        params = self.linear(x)\n",
    "        raw_scale = F.softplus(params[..., 0])\n",
    "        raw_shape = F.softplus(params[..., 1])\n",
    "        end_idx = torch.unsqueeze(sequence_lengths,-1)\n",
    "        scales_surv = torch.gather(raw_scale, dim=1, index=end_idx)[:,0]\n",
    "        shapes_surv = torch.gather(raw_shape, dim=1, index=end_idx)[:,0]\n",
    "        scales_events = raw_scale[:, :-1]\n",
    "        shapes_events = raw_shape[:, :-1]\n",
    "        tau_clamped = torch.clamp(tau, min=1e-6)\n",
    "        tau_cens_clamped = torch.clamp(tau_cens, min=1e-6)\n",
    "        log_prob = torch.log(shapes_events) - torch.log(scales_events) + (shapes_events - 1)*torch.log(tau_clamped) - (tau_clamped/scales_events)**shapes_events\n",
    "        log_surv = - (tau_cens_clamped/scales_surv)**shapes_surv\n",
    "        return log_prob, log_surv, raw_scale, raw_shape\n",
    "\n",
    "class NormalModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NormalModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.mean = nn.Linear(input_dim, output_dim)\n",
    "        self.vars = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        means = self.mean(x)\n",
    "        vars = self.vars(x)\n",
    "        log_prob = - 0.5*torch.log(vars[...,:-1, :])  - ((target - means[...,:-1, :]) ** 2) / (2 * vars[...,:-1, :])\n",
    "        return log_prob, means, vars\n",
    "\n",
    "class LogNormalModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogNormalModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lognormal_mean = nn.Linear(input_dim, output_dim)\n",
    "        self.lognormal_vars = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        means = self.lognormal_mean(x)\n",
    "        vars = self.lognormal_vars(x)\n",
    "        target_clamped = torch.clamp(target, min=1e-6)\n",
    "        log_prob = - 0.5*torch.log(vars[:,:-1, :])  - ((torch.log(target_clamped) - means[:,:-1, :]) ** 2) / (2 * vars[:,:-1, :]) - torch.log(target_clamped)\n",
    "        return log_prob, means, vars\n",
    "\n",
    "class SequenceEncoder(torch.nn.Module):\n",
    "    def __init__(self, starting_token_feature_dim : int, feature_dim: int, emb_dim: int, nhead: int, dim_feedforward: int, num_layers: int):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        self.s_token_linear = torch.nn.Linear(starting_token_feature_dim, emb_dim)\n",
    "        self.in_linear = torch.nn.Linear(feature_dim, emb_dim)\n",
    "        self.out_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.LayerNorm(emb_dim),\n",
    "        )\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    def forward(self, start_conditioner : torch.Tensor, x: torch.Tensor, sequence_lengths : torch.Tensor):\n",
    "        #starting_token = self.s_token_linear(start_conditioner).unsqueeze(1)\n",
    "        starting_token = torch.zeros((x.size(0), 1, self.in_linear.out_features), device=x.device)\n",
    "        output = self.in_linear(x)\n",
    "        output = torch.cat([starting_token, output], dim=1)\n",
    "        causal_mask = torch.nn.Transformer.generate_square_subsequent_mask(output.size(1), device=output.device, dtype=torch.bool)\n",
    "        padding_mask = (torch.arange(output.size(1), device=output.device).unsqueeze(0) >= (sequence_lengths+1).unsqueeze(1))\n",
    "        output = self.transformer_encoder(output, is_causal=True, mask=causal_mask, src_key_padding_mask=padding_mask)\n",
    "        output = self.out_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "31b3e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gam_filter(values, min_val, max_val):\n",
    "    rescaled_values = 2*(values - min_val) / (max_val - min_val) - 1.0\n",
    "    phi_values = np.arccos(rescaled_values) \n",
    "    return np.sin(phi_values[:,None] - phi_values[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8ed3037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_origin = pd.to_datetime(\"1997-01-01T00:00:00\").to_numpy()\n",
    "train_stop = pd.to_datetime(\"2020-01-01T00:00:00\").to_numpy()\n",
    "catalog = pd.read_csv(\"japanese-cat.csv\", sep=\" \", parse_dates=[\"Origin_Time(UT)\"])\n",
    "times = catalog[\"Origin_Time(UT)\"].to_numpy()\n",
    "times_days = (times - time_origin) / np.timedelta64(1, 'D')\n",
    "times = times_days.astype(np.float32)\n",
    "index_stop = np.argwhere(times >= (train_stop - time_origin) / np.timedelta64(1, 'D'))[0,0]\n",
    "magnitudes = catalog[\"JMA_Magnitude(Mj)\"].to_numpy()\n",
    "latitudes = catalog[\"Latitude(deg)\"].to_numpy()\n",
    "longitudes = catalog[\"Longitude(deg)\"].to_numpy()\n",
    "features = np.vstack([magnitudes, latitudes, longitudes]).T\n",
    "features[...,1:] = (features[...,1:] - features[:index_stop, 1:].mean(axis=0)) / features[:index_stop, 1:].std(axis=0)\n",
    "train_sequence = Sequence(arrival_times=times[:index_stop], features=features[:index_stop])\n",
    "test_sequence = Sequence(arrival_times=times[index_stop:], features=features[index_stop:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd61da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SequenceEncoder(starting_token_feature_dim=1, feature_dim=4, emb_dim=16, nhead=2, dim_feedforward=64, num_layers=1)\n",
    "wtmodel = WeibullWaitingTimeModel(input_dim=16)\n",
    "posmodel = NormalModel(input_dim=16, output_dim=2)\n",
    "magmodel = LogNormalModel(input_dim=16, output_dim=1)\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(wtmodel.parameters()) + list(posmodel.parameters()) + list(magmodel.parameters()), lr=1e-4)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []   \n",
    "for loop in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    subseq_inter_times, subseq_cens_inter_times, subseq_features, t_start, t_end = train_sequence.sample_sequences(\n",
    "        max_num_sequences=10,\n",
    "        duration_scale=15.0,\n",
    "        return_inter_times=True)\n",
    "    subseq_features = [ np.vstack([taus, feats.T]).T for taus, feats in zip(subseq_inter_times, subseq_features) ]\n",
    "    packed_features, sequence_lengths = Sequence.pack_sequences(\n",
    "        subseq_features)\n",
    "\n",
    "    packed_features = torch.tensor(packed_features, dtype=torch.float32)\n",
    "    sequence_lengths = torch.tensor(sequence_lengths, dtype=torch.long)\n",
    "    t_start = torch.tensor(t_start, dtype=torch.float32)\n",
    "    subseq_cens_inter_times = torch.tensor(subseq_cens_inter_times, dtype=torch.float32)\n",
    "    output = encoder(\n",
    "        torch.log(1+t_start[..., None]),   \n",
    "        packed_features, \n",
    "        sequence_lengths)\n",
    "\n",
    "    log_prob, log_surv, raw_scale, raw_shape = wtmodel(output, packed_features[...,0],\n",
    "                                  subseq_cens_inter_times, sequence_lengths)\n",
    "    log_prob_mask = (torch.arange(log_prob.size(1))[None, :] < sequence_lengths[:, None]).to(log_prob.device)\n",
    "\n",
    "    \n",
    "    pos_log_prob, pos_means, pos_vars = posmodel(output, packed_features[...,2:])\n",
    "    mag_log_prob, mag_means, mag_vars = magmodel(output, packed_features[...,1:2])\n",
    "\n",
    "    \n",
    "    loss = - (log_prob * log_prob_mask).sum(-1) - log_surv\n",
    "    loss += - (pos_log_prob * log_prob_mask[:,:,None]).sum([-2,-1])\n",
    "    loss += - (mag_log_prob * log_prob_mask[:,:,None]).sum([-2,-1])\n",
    "    if(torch.isnan(loss).any()):\n",
    "        print(\"NaN encountered in loss computation at step \", loop)\n",
    "        break\n",
    "    loss = loss.mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if loop % 100 == 0:\n",
    "        print(f\"Loop {loop}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_samples = 100\n",
    "num_steps = 20\n",
    "sampling_origin = index_stop + 100\n",
    "input_t_start = times[sampling_origin - 1]\n",
    "input_feature = features[sampling_origin]\n",
    "input_tau = times[sampling_origin] - input_t_start\n",
    "input_tensor = torch.tensor(np.hstack([input_tau, input_feature])[None, None, :], dtype=torch.float32)\n",
    "input_tensor = torch.repeat_interleave(input_tensor, repeats=num_input_samples, dim=0)\n",
    "input_t_start = torch.tensor(np.array([input_t_start]*num_input_samples), dtype=torch.float32)\n",
    "input_sequence_length = torch.tensor([1]*num_input_samples, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(num_steps):\n",
    "        output = encoder(\n",
    "                torch.log(1+input_t_start[..., None]),   \n",
    "                input_tensor, \n",
    "                input_sequence_length)\n",
    "\n",
    "        log_prob, log_surv, raw_scale, raw_shape = wtmodel(output, input_tensor[...,0],\n",
    "                                    torch.tensor([np.inf]*num_input_samples, dtype=torch.float32),\n",
    "                                    input_sequence_length)\n",
    "\n",
    "        pos_log_prob, pos_means, pos_vars = posmodel(output, input_tensor[...,2:])\n",
    "        mag_log_prob, mag_means, mag_vars = magmodel(output, input_tensor[...,1:2])\n",
    "\n",
    "        new_event_waiting_times = torch.zeros((num_input_samples,))\n",
    "        new_event_waiting_times = (new_event_waiting_times.exponential_()/raw_scale[:,-1])**(raw_shape[:,-1])\n",
    "\n",
    "        new_positions = torch.normal(pos_means[:,-1,:], torch.sqrt(pos_vars[:,-1,:]))\n",
    "        new_magnitudes = torch.exp(torch.normal(mag_means[:,-1,:], torch.sqrt(mag_vars[:,-1,:])))\n",
    "\n",
    "        new_features = torch.hstack([new_event_waiting_times.unsqueeze(-1), new_magnitudes, new_positions])[:, None, :]\n",
    "        input_tensor = torch.cat([input_tensor, new_features], dim=1)\n",
    "        input_sequence_length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85761f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "for sample_idx in range(num_input_samples):\n",
    "    plt.plot(times[sampling_origin:sampling_origin+10*num_steps] - times[sampling_origin-1], features[sampling_origin:sampling_origin+10*num_steps,0])\n",
    "    plt.plot(torch.cumsum(input_tensor[sample_idx,:, 0], dim=0).numpy(), input_tensor[sample_idx,:, 1].numpy(), color='red', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ca669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
