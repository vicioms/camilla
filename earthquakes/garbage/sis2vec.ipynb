{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da339fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import obspy\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from torchvision.ops.misc import ConvNormActivation\n",
    "from torchvision.ops import StochasticDepth\n",
    "from typing import List, Optional, Union, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77415c",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf889a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MAE-1D FIXES (canonical ids_keep/ids_restore, no ragged padding, correct LayerScale, patch regression) =====\n",
    "# Drop-in replacements for: TransformerLayerRoPE, MAEEncoder1d, MAEDecoder1d\n",
    "# + helper functions for MAE masking and patchify/unpatchify targets.\n",
    "\n",
    "from einops import rearrange\n",
    "from rotary_embedding_torch import RotaryEmbedding, apply_rotary_emb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: patchify + MAE mask\n",
    "# ----------------------------\n",
    "def patchify_1d(x: torch.Tensor, patch_size: int, pad_to_patch: bool = True) -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    x: (B, C, L)\n",
    "    returns:\n",
    "      patches: (B, N, C*P)\n",
    "      pad: int padding applied on the right\n",
    "    \"\"\"\n",
    "    B, C, L = x.shape\n",
    "    P = patch_size\n",
    "    pad = 0\n",
    "    if pad_to_patch:\n",
    "        pad = (-L) % P\n",
    "        if pad:\n",
    "            x = F.pad(x, (0, pad))\n",
    "            L = L + pad\n",
    "    else:\n",
    "        assert L % P == 0, f\"L={L} not divisible by patch_size={P}\"\n",
    "\n",
    "    N = L // P\n",
    "    patches = x.unfold(2, P, P)                    # (B, C, N, P)\n",
    "    patches = patches.permute(0, 2, 1, 3)          # (B, N, C, P)\n",
    "    patches = patches.contiguous().view(B, N, C * P)\n",
    "    return patches, pad\n",
    "\n",
    "\n",
    "def mae_random_masking(B: int, N: int, mask_ratio: float, device=None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Canonical MAE masking.\n",
    "    Returns:\n",
    "      ids_keep:    (B, N_keep) indices of visible tokens in shuffled order\n",
    "      ids_restore: (B, N) inverse permutation to restore original order\n",
    "      mask:        (B, N) float mask in original order: 1=masked, 0=visible\n",
    "    \"\"\"\n",
    "    assert 0.0 <= mask_ratio < 1.0\n",
    "    N_keep = int(round(N * (1.0 - mask_ratio)))\n",
    "\n",
    "    noise = torch.rand(B, N, device=device)\n",
    "    ids_shuffle = noise.argsort(dim=1)           # (B, N)\n",
    "    ids_restore = ids_shuffle.argsort(dim=1)     # (B, N)\n",
    "\n",
    "    ids_keep = ids_shuffle[:, :N_keep]           # (B, N_keep)\n",
    "\n",
    "    mask = torch.ones(B, N, device=device)\n",
    "    mask[:, :N_keep] = 0\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)  # unshuffle to original order\n",
    "    return ids_keep, ids_restore, mask\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Patch encoder (unchanged)\n",
    "# ----------------------------\n",
    "class ConvPatchEncoder1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        emb_dim: int,\n",
    "        num_conv_layers: int,\n",
    "        patch_size: int,\n",
    "        kernel_size: int = 3,\n",
    "        padding: int = 1,\n",
    "        stride: int = 1,\n",
    "        pad_to_patch: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.pad_to_patch = pad_to_patch\n",
    "\n",
    "        layers = []\n",
    "        c_in = in_channels\n",
    "        for _ in range(num_conv_layers):\n",
    "            layers.append(nn.Conv1d(c_in, emb_dim, kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "            layers.append(nn.GELU())\n",
    "            c_in = emb_dim\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, C, L)\n",
    "        B, C, L = x.shape\n",
    "        P = self.patch_size\n",
    "\n",
    "        if self.pad_to_patch:\n",
    "            pad = (-L) % P\n",
    "            if pad:\n",
    "                x = F.pad(x, (0, pad))\n",
    "                L = L + pad\n",
    "        else:\n",
    "            assert L % P == 0, f\"L={L} not divisible by patch_size={P}\"\n",
    "\n",
    "        N = L // P\n",
    "\n",
    "        x = x.unfold(dimension=2, size=P, step=P)           # (B, C, N, P)\n",
    "        x = rearrange(x, 'b c n p -> (b n) c p')            # (B*N, C, P)\n",
    "\n",
    "        x = self.net(x)                                     # (B*N, emb_dim, p')\n",
    "        x = self.pool(x).squeeze(-1)                        # (B*N, emb_dim)\n",
    "        x = self.norm(x)                                    # (B*N, emb_dim)\n",
    "\n",
    "        x = rearrange(x, '(b n) d -> b n d', b=B, n=N)       # (B, N, emb_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# FIXED Transformer layer (LayerScale once, proper RoPE positions, optional attn_mask)\n",
    "# --------------------------------\n",
    "class TransformerLayerRoPE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim: int,\n",
    "        nheads: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        layer_scale: float = 1e-2,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert emb_dim % nheads == 0, \"emb_dim must be divisible by nheads\"\n",
    "        self.emb_dim = emb_dim\n",
    "        self.nheads = nheads\n",
    "        self.head_dim = emb_dim // nheads\n",
    "\n",
    "        self.pos_emb = RotaryEmbedding(dim=self.head_dim)\n",
    "\n",
    "        self.q_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, dim_feedforward, bias=bias),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, emb_dim, bias=bias),\n",
    "        )\n",
    "\n",
    "        self.ls1 = nn.Parameter(layer_scale * torch.ones(emb_dim)) if layer_scale > 0 else None\n",
    "        self.ls2 = nn.Parameter(layer_scale * torch.ones(emb_dim)) if layer_scale > 0 else None\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def _shape_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (B, N, D) -> (B, H, N, Dh)\n",
    "        B, N, D = x.shape\n",
    "        return x.view(B, N, self.nheads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def _merge_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (B, H, N, Dh) -> (B, N, D)\n",
    "        B, H, N, Dh = x.shape\n",
    "        return x.transpose(1, 2).contiguous().view(B, N, H * Dh)\n",
    "\n",
    "    def _apply_rope(self, q: torch.Tensor, k: torch.Tensor, positions: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        q,k: (B,H,N,Dh)\n",
    "        positions:\n",
    "          - None -> sequential [0..N-1]\n",
    "          - (B,N) long -> absolute positions per token\n",
    "        \"\"\"\n",
    "        if positions is None:\n",
    "            q = self.pos_emb.rotate_queries_or_keys(q)\n",
    "            k = self.pos_emb.rotate_queries_or_keys(k)\n",
    "            return q, k\n",
    "\n",
    "        # positions are per-batch absolute indices\n",
    "        # rotary_embedding_torch: pos_emb.forward(t) gives freqs for those positions\n",
    "        # Need positions as int/long; build freqs for all needed positions then index\n",
    "        max_pos = int(positions.max().item())\n",
    "        pos = torch.arange(max_pos + 1, device=positions.device)        # (max_pos+1,)\n",
    "        freqs = self.pos_emb.forward(pos)                               # (max_pos+1, Dh)\n",
    "        freqs_batch = freqs[positions]                                  # (B, N, Dh)\n",
    "        freqs_batch = freqs_batch.unsqueeze(1)                          # (B, 1, N, Dh)\n",
    "\n",
    "        q = apply_rotary_emb(freqs_batch, q)\n",
    "        k = apply_rotary_emb(freqs_batch, k)\n",
    "        return q, k\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        positions: Optional[torch.Tensor] = None,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        src: (B, N, D)\n",
    "        positions: None or (B,N) long absolute indices for RoPE\n",
    "        attn_mask: passed to scaled_dot_product_attention; bool mask uses True=allowed\n",
    "                  typical key padding mask (True=keep keys): (B,1,1,N) broadcastable\n",
    "        \"\"\"\n",
    "        # ---- attention (pre-norm) ----\n",
    "        x = self.norm1(src)\n",
    "        q = self._shape_heads(self.q_proj(x))\n",
    "        k = self._shape_heads(self.k_proj(x))\n",
    "        v = self._shape_heads(self.v_proj(x))\n",
    "\n",
    "        q, k = self._apply_rope(q, k, positions)\n",
    "\n",
    "        attn_out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=attn_mask,\n",
    "            dropout_p=self.drop.p if self.training else 0.0,\n",
    "            is_causal=False,\n",
    "        )\n",
    "        attn_out = self._merge_heads(attn_out)\n",
    "        attn_out = self.out_proj(attn_out)\n",
    "\n",
    "        if self.ls1 is not None:\n",
    "            src = src + self.drop(attn_out) * self.ls1[None, None, :]\n",
    "        else:\n",
    "            src = src + self.drop(attn_out)\n",
    "\n",
    "        # ---- ffn (pre-norm) ----\n",
    "        y = self.ffn(self.norm2(src))\n",
    "\n",
    "        # FIX: apply LayerScale ONLY ONCE (not twice)\n",
    "        if self.ls2 is not None:\n",
    "            src = src + self.drop(y) * self.ls2[None, None, :]\n",
    "        else:\n",
    "            src = src + self.drop(y)\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# FIXED MAE encoder: canonical ids_keep, no ragged padding, correct RoPE positions\n",
    "# ----------------------------\n",
    "class MAEEncoder1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim: int,\n",
    "        nheads: int,\n",
    "        num_layers: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        layer_scale: float = 1e-2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayerRoPE(\n",
    "                emb_dim=emb_dim,\n",
    "                nheads=nheads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                layer_scale=layer_scale,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask_ratio: Optional[float] = None,\n",
    "        ids_keep: Optional[torch.Tensor] = None,\n",
    "        ids_restore: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        x: (B, N, D)\n",
    "        Either provide:\n",
    "          - mask_ratio (and it will sample ids_keep/ids_restore), or\n",
    "          - ids_keep + ids_restore (precomputed)\n",
    "\n",
    "        Returns:\n",
    "          z_vis:      (B, N_keep, D) encoded visible tokens\n",
    "          ids_keep:   (B, N_keep)\n",
    "          ids_restore:(B, N)\n",
    "          mask:       (B, N) float, 1=masked, 0=visible\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        if ids_keep is None:\n",
    "            assert mask_ratio is not None, \"Provide mask_ratio or ids_keep/ids_restore\"\n",
    "            ids_keep, ids_restore, mask = mae_random_masking(B, N, mask_ratio, device=x.device)\n",
    "        else:\n",
    "            assert ids_restore is not None, \"If ids_keep is provided, ids_restore must be provided\"\n",
    "            mask = None\n",
    "\n",
    "        # gather visible tokens\n",
    "        x_vis = x.gather(1, ids_keep[..., None].expand(-1, -1, D))  # (B, N_keep, D)\n",
    "        pos_vis = ids_keep                                           # (B, N_keep) absolute positions\n",
    "\n",
    "        # no padding needed; all have same N_keep\n",
    "        for layer in self.layers:\n",
    "            x_vis = layer(x_vis, positions=pos_vis, attn_mask=None)\n",
    "\n",
    "        return x_vis, ids_keep, ids_restore, mask\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# FIXED MAE decoder: append mask tokens, unshuffle with ids_restore, predict patch values\n",
    "# ----------------------------\n",
    "class MAEDecoder1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        enc_dim: int,\n",
    "        dec_dim: int,\n",
    "        patch_dim: int,          # = in_channels * patch_size\n",
    "        nheads: int,\n",
    "        num_layers: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        layer_scale: float = 1e-2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dec_dim = dec_dim\n",
    "\n",
    "        self.dec_embed = nn.Linear(enc_dim, dec_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, dec_dim))\n",
    "        nn.init.normal_(self.mask_token, std=0.02)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayerRoPE(\n",
    "                emb_dim=dec_dim,\n",
    "                nheads=nheads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                layer_scale=layer_scale,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(dec_dim)\n",
    "        self.pred = nn.Linear(dec_dim, patch_dim)\n",
    "\n",
    "    def forward(self, z_vis: torch.Tensor, ids_keep: torch.Tensor, ids_restore: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        z_vis: (B, N_keep, enc_dim)\n",
    "        ids_keep: (B, N_keep)\n",
    "        ids_restore: (B, N)\n",
    "        returns:\n",
    "          pred_patches: (B, N, patch_dim)\n",
    "        \"\"\"\n",
    "        B, N_keep, _ = z_vis.shape\n",
    "        N = ids_restore.shape[1]\n",
    "\n",
    "        z_vis = self.dec_embed(z_vis)                    # (B, N_keep, dec_dim)\n",
    "\n",
    "        # append mask tokens to reach length N (in shuffled order: [keep | mask])\n",
    "        n_mask = N - N_keep\n",
    "        z_mask = self.mask_token.expand(B, n_mask, self.dec_dim)\n",
    "        z_ = torch.cat([z_vis, z_mask], dim=1)          # (B, N, dec_dim) in shuffled order\n",
    "\n",
    "        # unshuffle to original order\n",
    "        z_full = z_.gather(1, ids_restore[..., None].expand(-1, -1, self.dec_dim))  # (B, N, dec_dim)\n",
    "\n",
    "        # decode with sequential positions (positions=None -> 0..N-1)\n",
    "        for layer in self.layers:\n",
    "            z_full = layer(z_full, positions=None, attn_mask=None)\n",
    "\n",
    "        z_full = self.norm(z_full)\n",
    "        pred = self.pred(z_full)                         # (B, N, patch_dim)\n",
    "        return pred\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Full MAE wrapper (optional convenience)\n",
    "# ----------------------------\n",
    "class MAE1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        patch_size: int,\n",
    "        enc_dim: int,\n",
    "        dec_dim: int,\n",
    "        enc_layers: int,\n",
    "        dec_layers: int,\n",
    "        nheads_enc: int,\n",
    "        nheads_dec: int,\n",
    "        enc_dim_ff: int,\n",
    "        dec_dim_ff: int,\n",
    "        dropout: float = 0.1,\n",
    "        layer_scale: float = 1e-2,\n",
    "        num_conv_layers: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_dim = in_channels * patch_size\n",
    "\n",
    "        self.patch_encoder = ConvPatchEncoder1d(\n",
    "            in_channels=in_channels,\n",
    "            emb_dim=enc_dim,\n",
    "            num_conv_layers=num_conv_layers,\n",
    "            patch_size=patch_size,\n",
    "        )\n",
    "\n",
    "        self.encoder = MAEEncoder1d(\n",
    "            emb_dim=enc_dim,\n",
    "            nheads=nheads_enc,\n",
    "            num_layers=enc_layers,\n",
    "            dim_feedforward=enc_dim_ff,\n",
    "            dropout=dropout,\n",
    "            layer_scale=layer_scale,\n",
    "        )\n",
    "\n",
    "        self.decoder = MAEDecoder1d(\n",
    "            enc_dim=enc_dim,\n",
    "            dec_dim=dec_dim,\n",
    "            patch_dim=self.patch_dim,\n",
    "            nheads=nheads_dec,\n",
    "            num_layers=dec_layers,\n",
    "            dim_feedforward=dec_dim_ff,\n",
    "            dropout=dropout,\n",
    "            layer_scale=layer_scale,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask_ratio: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: (B, C, L) normalized traces\n",
    "        returns:\n",
    "          loss, pred_patches, mask (B,N) with 1=masked\n",
    "        \"\"\"\n",
    "        # targets in input space\n",
    "        target_patches, _ = patchify_1d(x, self.patch_size, pad_to_patch=True)   # (B,N,patch_dim)\n",
    "\n",
    "        tokens = self.patch_encoder(x)                                           # (B,N,enc_dim)\n",
    "        z_vis, ids_keep, ids_restore, mask = self.encoder(tokens, mask_ratio=mask_ratio)\n",
    "\n",
    "        pred = self.decoder(z_vis, ids_keep, ids_restore)                        # (B,N,patch_dim)\n",
    "\n",
    "        # MAE loss: masked-only\n",
    "        # mask: (B,N) float 1=masked 0=visible\n",
    "        loss_map = (pred - target_patches).pow(2).mean(dim=-1)                   # (B,N)\n",
    "        loss = (loss_map * mask).sum() / mask.sum().clamp_min(1.0)\n",
    "\n",
    "        return loss, pred, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteadDataset(Dataset):\n",
    "\n",
    "    def __init__(self, chunk_files, channel_first):\n",
    "        self.files = []\n",
    "        self.event_lists = []\n",
    "        self.stopping_indices = None\n",
    "        for chunk in chunk_files:\n",
    "            file = h5py.File(chunk, 'r')\n",
    "            metadata = pd.read_csv(chunk.replace('hdf5', 'csv'))\n",
    "            ev_list = metadata['trace_name'].astype('str').to_list()\n",
    "            self.files.append(file)\n",
    "            self.event_lists.append(ev_list)\n",
    "            if self.stopping_indices:\n",
    "                self.stopping_indices.append(self.stopping_indices[-1] + len(ev_list))\n",
    "            else:\n",
    "                self.stopping_indices = [len(ev_list)]\n",
    "        self.stopping_indices = np.array(self.stopping_indices)\n",
    "        self.channel_first = channel_first\n",
    "    def __len__(self):\n",
    "        return sum([len(ev_list) for ev_list in self.event_lists])\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # find which chunk\n",
    "        chunk_idx = 0\n",
    "        while idx >= self.stopping_indices[chunk_idx]:\n",
    "            chunk_idx += 1\n",
    "        relative_idx = idx - self.stopping_indices[chunk_idx - 1] if chunk_idx > 0 else idx\n",
    "        event_name = self.event_lists[chunk_idx][relative_idx]\n",
    "        file = self.files[chunk_idx].get('data/' + event_name)\n",
    "        trace = np.array(file)\n",
    "        p_arrival = file.attrs['p_arrival_sample']\n",
    "        s_arrival = file.attrs['s_arrival_sample']\n",
    "        coda_end = file.attrs['coda_end_sample']\n",
    "        if(p_arrival == ''):\n",
    "            p_arrival = np.nan\n",
    "        if(s_arrival == ''):\n",
    "            s_arrival = np.nan\n",
    "        if(coda_end == ''):\n",
    "            coda_end = np.nan\n",
    "        if self.channel_first:\n",
    "            trace = trace.transpose(1, 0)\n",
    "        return trace, p_arrival.item(), s_arrival.item(), coda_end.item(), event_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af647a04",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a858be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'STEAD/'\n",
    "train_chunks = [ root + f'chunk{chunk}/chunk{chunk}.hdf5'  for chunk in range(2, 4) ]\n",
    "val_chunks = [ root + f'chunk{chunk}/chunk{chunk}.hdf5'  for chunk in range(4, 5) ]\n",
    "train_dataset = SteadDataset(train_chunks , channel_first=True)\n",
    "val_dataset = SteadDataset(val_chunks , channel_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7cc252",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0) # must be 0 for hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec02e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- hyperparams ---\n",
    "patch_size = 80\n",
    "in_channels = 3\n",
    "mask_ratio = 0.4\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ---- build model (using the FIXED classes) ----\n",
    "mae = MAE1d(\n",
    "    in_channels=in_channels,\n",
    "    patch_size=patch_size,\n",
    "    enc_dim=128,\n",
    "    dec_dim=64,\n",
    "    enc_layers=4,\n",
    "    dec_layers=2,\n",
    "    nheads_enc=8,\n",
    "    nheads_dec=4,\n",
    "    enc_dim_ff=512,\n",
    "    dec_dim_ff=256,\n",
    "    dropout=0.1,\n",
    "    layer_scale=1e-2,\n",
    "    num_conv_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    mae.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2\n",
    ")\n",
    "\n",
    "# If you want cosine over *all steps*\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\n",
    "# Optional but recommended for stability\n",
    "grad_clip = 1.0\n",
    "\n",
    "mae.train()\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (traces, p_arrivals, s_arrivals, coda_ends, event_names) in enumerate(train_loader):\n",
    "        traces = traces.to(device)  # (B,3,L)\n",
    "\n",
    "        # normalize per trace (your choice; keep for now)\n",
    "        traces_mean = traces.mean(dim=2, keepdim=True)\n",
    "        traces_std  = traces.std(dim=2, keepdim=True).clamp_min(1e-9)\n",
    "        normalized_traces = (traces - traces_mean) / traces_std\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # ---- MAE forward: returns masked-only loss already ----\n",
    "        loss, pred_patches, mask = mae(normalized_traces, mask_ratio=mask_ratio)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if grad_clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(mae.parameters(), grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # mask is 1=masked; print masked fraction\n",
    "            print(f\"Epoch {epoch} Step {i}  Loss {loss.item():.3e}  masked_frac {mask.mean().item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fa542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDirichletRegression(nn.Module):\n",
    "    def __init__(self,  input_dim, output_dim, kernel_sizes, channel_sizes, strides, paddings):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        for i, (k, s, p) in enumerate(zip(kernel_sizes, strides, paddings)):\n",
    "            conv = nn.Conv1d(\n",
    "                in_channels=input_dim if i == 0 else channel_sizes[i-1],\n",
    "                out_channels=channel_sizes[i],\n",
    "                kernel_size=k,\n",
    "                stride=s,\n",
    "                padding=p,\n",
    "            )\n",
    "            self.net.add_module(f\"conv_{i}\", conv)\n",
    "            self.net.add_module(f\"norm_{i}\", nn.GroupNorm(num_groups=channel_sizes[i], num_channels=channel_sizes[i]))\n",
    "            self.net.add_module(f\"gelu_{i}\", nn.GELU())\n",
    "\n",
    "        self.output_conv = nn.Conv1d(\n",
    "            in_channels=channel_sizes[-1],\n",
    "            out_channels=channel_sizes[-1],\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0)\n",
    "\n",
    "        self.output_alphas = nn.Linear(channel_sizes[-1], output_dim)\n",
    "        self.output_alpha0 = nn.Linear(channel_sizes[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.output_conv(x)\n",
    "        x = x.mean(dim=2)  # global average pooling over time dimension\n",
    "        alpha_scores = self.output_alphas(x)\n",
    "        alpha0 = F.softplus(self.output_alpha0(x))\n",
    "        return F.softmax(alpha_scores, dim=-1) * alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_batch_size = 64\n",
    "downstream_num_epochs = 10\n",
    "downstream_train_loader = DataLoader(train_dataset, batch_size=downstream_batch_size, shuffle=True, num_workers=0) # must be 0 for hdf5\n",
    "interval_model = LatentDirichletRegression(\n",
    "    input_dim=128,\n",
    "    output_dim=4,\n",
    "    kernel_sizes=[3,3,3],\n",
    "    channel_sizes=[128,64,32],\n",
    "    strides=[2,2,2],\n",
    "    paddings=[1,1,1],).to(device)\n",
    "downstream_optimizer = optim.AdamW(\n",
    "    interval_model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2\n",
    ")\n",
    "downstream_scheduler = optim.lr_scheduler.CosineAnnealingLR(downstream_optimizer, T_max=downstream_num_epochs * len(downstream_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae.eval()\n",
    "interval_model.train()\n",
    "for epoch in range(downstream_num_epochs):\n",
    "    for i, (traces, p_arrivals, s_arrivals, coda_ends, event_names) in enumerate(downstream_train_loader):\n",
    "        downstream_optimizer.zero_grad()\n",
    "        traces = traces.to(device)\n",
    "        traces_mean = traces.mean(dim=2, keepdim=True)\n",
    "        traces_std = traces.std(dim=2, keepdim=True) + 1e-9\n",
    "        normalized_traces = (traces - traces_mean) / traces_std  # normalize input traces\n",
    "        num_timesteps = traces.size(-1)\n",
    "        with torch.no_grad():\n",
    "            tokens = mae.patch_encoder(normalized_traces)\n",
    "            embeddings = mae.encoder(tokens)[0] # (B, Np, D)\n",
    "        alphas = interval_model(embeddings.transpose(1, 2))  # (B, T_out)\n",
    "        dist = torch.distributions.Dirichlet(alphas + 1e-9)\n",
    "        s1 = p_arrivals/num_timesteps\n",
    "        s2 = s_arrivals/num_timesteps - s1\n",
    "        s3 = coda_ends/num_timesteps - s1 - s2\n",
    "        s4 = 1.0 - (s1 + s2 + s3)\n",
    "        target = torch.stack([s1, s2, s3, s4], dim=-1)\n",
    "        loss = -dist.log_prob(target.to(device)).mean()\n",
    "        loss.backward()\n",
    "        downstream_optimizer.step()\n",
    "        downstream_scheduler.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Downstream Epoch {epoch}, Step {i}, Loss: {loss.item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6222202",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dNormActivation(ConvNormActivation):\n",
    "    \"\"\"\n",
    "    Configurable block used for Convolution2d-Normalization-Activation blocks.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input image\n",
    "        out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block\n",
    "        kernel_size: (int, optional): Size of the convolving kernel. Default: 3\n",
    "        stride (int, optional): Stride of the convolution. Default: 1\n",
    "        padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will be calculated as ``padding = (kernel_size - 1) // 2 * dilation``\n",
    "        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer won't be used. Default: ``torch.nn.BatchNorm2d``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        dilation (int): Spacing between kernel elements. Default: 1\n",
    "        inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True``\n",
    "        bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        padding: Optional[Union[int, tuple[int, int], str]] = None,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm1d,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        dilation: int = 1,\n",
    "        inplace: Optional[bool] = True,\n",
    "        bias: Optional[bool] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            groups,\n",
    "            norm_layer,\n",
    "            activation_layer,\n",
    "            dilation,\n",
    "            inplace,\n",
    "            bias,\n",
    "            torch.nn.Conv1d,\n",
    "        )\n",
    "class GlobalResponseNorm1d(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super(GlobalResponseNorm1d, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.zeros(1,1,dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1,1,dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the global response norm\n",
    "        gx = torch.norm(x, p=2, dim=1, keepdim=True)\n",
    "        nx = gx/(torch.mean(gx, dim=-1, keepdim=True) + self.eps)\n",
    "        return self.gamma * (x * nx) + self.beta + x\n",
    "class LayerNorm1d(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # NCL -> NLC\n",
    "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        x = x.permute(0, 2, 1)  # NLC -> NCL\n",
    "        return x\n",
    "class CN2Upsample1d(nn.Module):\n",
    "    def __init__(self, in_dim : int, out_dim : int):\n",
    "        super(CN2Upsample1d, self).__init__()\n",
    "        self.norm = LayerNorm1d(in_dim)\n",
    "        self.conv = nn.ConvTranspose1d(in_dim, out_dim, kernel_size=2, stride=2, bias=True)\n",
    "    def forward(self, input):\n",
    "        x = self.norm(input)\n",
    "        return self.conv(x)\n",
    "class CN2Downsample1d(nn.Module):\n",
    "    def __init__(self, in_dim : int, out_dim : int):\n",
    "        super(CN2Downsample1d, self).__init__()\n",
    "        self.norm = LayerNorm1d(in_dim)\n",
    "        self.conv = nn.Conv1d(in_dim, out_dim, kernel_size=2, stride=2, bias=True)\n",
    "    def forward(self, input):\n",
    "        x = self.norm(input)\n",
    "        return self.conv(x)\n",
    "class CN2Block1d(nn.Module):\n",
    "    def __init__(self, dim: int, stochastic_depth_prob: float):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(dim, dim, kernel_size=7, stride=1, padding=3, groups=dim, bias=True)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.linear1 = nn.Linear(dim, 4 * dim, bias=True)\n",
    "        self.grn = GlobalResponseNorm1d(4 * dim)\n",
    "        self.linear2 = nn.Linear(4 * dim, dim, bias=True)\n",
    "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
    "\n",
    "    def _block(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # x: (N,C,L), mask: (N,1,L) where True=masked\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 1:\n",
    "                mask = mask[None, None, :]\n",
    "            elif mask.dim() == 2:\n",
    "                mask = mask[:, None, :]\n",
    "            x = x.masked_fill(mask, 0.0)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if mask is not None:\n",
    "            x = x.masked_fill(mask, 0.0)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # NCL -> NLC\n",
    "        x = self.norm1(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.grn(x)\n",
    "        x = self.linear2(x)\n",
    "        x = x.permute(0, 2, 1)  # NLC -> NCL\n",
    "\n",
    "        x = self.stochastic_depth(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 1:\n",
    "                mask = mask[None, None, :]\n",
    "            elif mask.dim() == 2:\n",
    "                mask = mask[:, None, :]\n",
    "            x = x.masked_fill(mask, 0.0)\n",
    "        return x + self._block(x, mask)\n",
    "class CNBlock1d(nn.Module):\n",
    "    def __init__(self, dim : int):\n",
    "        super(CNBlock1d, self).__init__()\n",
    "        # this does not change the input size and dim\n",
    "        self.conv1 = nn.Conv1d(dim, dim, kernel_size=7, stride=1, padding=3, groups=dim, bias=True)\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.linear1 =nn.Linear(dim, 4*dim, bias=True)\n",
    "        self.linear2 =nn.Linear(4*dim, dim, bias=True)\n",
    "        self.layer_scale = nn.Parameter(1e-6 * torch.ones((dim)), requires_grad=True)\n",
    "    def _block(self, input: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = self.conv1(input)\n",
    "        x = x.permute(0, 2, 1)  # NCL -> NLC\n",
    "        x = self.norm1(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = x * self.layer_scale\n",
    "        x = x.permute(0, 2, 1)  # NLC -> NCL\n",
    "        return x\n",
    "    def forward(self, input: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        return input + self._block(input, mask)\n",
    "    \n",
    "\n",
    "class CN2Encoder1d(nn.Module):\n",
    "    def __init__(self, input_channels : int,\n",
    "                embed_dim : int,\n",
    "                num_blocks : List[int],\n",
    "                stochastic_depth_prob : float):\n",
    "        super(CN2Encoder1d, self).__init__()\n",
    "        self.stem = Conv1dNormActivation(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=4,\n",
    "            padding=0,\n",
    "            stride=4,\n",
    "            norm_layer=LayerNorm1d,\n",
    "            activation_layer=None,\n",
    "            bias=True\n",
    "        )\n",
    "        self.stem_pooler = nn.AvgPool1d(\n",
    "            kernel_size=4,\n",
    "            stride=4\n",
    "        )\n",
    "        self.down_pooler = nn.AvgPool1d(\n",
    "            kernel_size=2,\n",
    "            stride=2\n",
    "        )\n",
    "        num_stages = len(num_blocks)\n",
    "        self.num_stages = num_stages\n",
    "        self.stages = nn.ModuleList()\n",
    "        self.downsamples = nn.ModuleList()\n",
    "        for i in range(num_stages):\n",
    "            stage = nn.ModuleList()\n",
    "            for j in range(num_blocks[i]):\n",
    "                block = CN2Block1d(\n",
    "                    dim=embed_dim,\n",
    "                    stochastic_depth_prob=stochastic_depth_prob\n",
    "                )\n",
    "                stage.append(block)\n",
    "            \n",
    "            self.stages.append(stage)\n",
    "            if i < num_stages-1:\n",
    "                downsample = CN2Downsample1d(\n",
    "                    in_dim=embed_dim,\n",
    "                    out_dim=embed_dim\n",
    "                )\n",
    "                self.downsamples.append(downsample)\n",
    "\n",
    "    def get_downscale_factor(self) -> int:\n",
    "        return 4 * (2 ** (len(self.downsamples)))  # stem + downsamples        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # x: (N,C,L)\n",
    "        if mask is not None:\n",
    "            # accetta mask (L,) oppure (N,L) oppure (N,1,L)\n",
    "            if mask.dim() == 1:\n",
    "                mask = mask[None, :]          # -> (1,L)\n",
    "            elif mask.dim() == 2:\n",
    "                mask = mask[:, None]             # -> (N,1,L)\n",
    "            mask = mask.to(device=x.device, dtype=torch.bool)\n",
    "\n",
    "            # applica su input (broadcast su C)\n",
    "            x = x.masked_fill(mask, 0.0)\n",
    "\n",
    "        # stem: stride=4\n",
    "        x = self.stem(x)\n",
    "\n",
    "        if mask is not None:\n",
    "            # pool mask in modo coerente con lo stem (stride=4)\n",
    "            x_mask = self.stem_pooler(mask.float()).to(dtype=torch.bool)\n",
    "            x = x.masked_fill(x_mask, 0.0)  # broadcasting su C\n",
    "            #print(\"after stem\", x_mask.shape, x.shape)\n",
    "\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            for block in stage:\n",
    "                x = block(x)\n",
    "                if mask is not None:\n",
    "                    x = x.masked_fill(x_mask, 0.0)\n",
    "                #print(f\"after stage {i}\", x_mask.shape, x.shape)\n",
    "            if i < len(self.downsamples):\n",
    "                x = self.downsamples[i](x)  # stride=2\n",
    "\n",
    "                if mask is not None:\n",
    "                    # pool mask con stride=2 (coerente col downsample)\n",
    "                    x_mask = self.down_pooler(x_mask.float()).to(dtype=torch.bool)\n",
    "                    x = x.masked_fill(x_mask, 0.0)\n",
    "                    #print(f\"after down {i}\", x_mask.shape, x.shape)\n",
    "        return x\n",
    "    \n",
    "class CN2Decoder1d(nn.Module):\n",
    "    def __init__(self,embed_dim : int, out_channels : int, num_upsamples : int):\n",
    "        super(CN2Decoder1d, self).__init__()\n",
    "        self.upsampler = nn.Sequential(*[\n",
    "             nn.Sequential(CN2Upsample1d(embed_dim, embed_dim), LayerNorm1d(embed_dim), nn.GELU())\n",
    "        for _ in range(num_upsamples) ])\n",
    "        self.block = CNBlock1d(embed_dim)\n",
    "        self.final_conv = nn.Conv1d(embed_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (N,C,L)\n",
    "        x = self.upsampler(x)\n",
    "        x = self.block(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time is the last dimension\n",
    "class ConvFeatureEncoder(nn.Module):\n",
    "    def __init__(self, in_ch, dim, kernel_sizes, strides, paddings):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        for i, (k, s, p) in enumerate(zip(kernel_sizes, strides, paddings)):\n",
    "            conv = nn.Conv1d(in_ch if i == 0 else dim, dim, kernel_size=k, stride=s, padding=p)\n",
    "            self.net.add_module(f\"conv_{i}\", conv)\n",
    "            self.net.add_module(f\"gelu_{i}\", nn.GELU())\n",
    "    def forward(self, x):  \n",
    "        return self.net(x)\n",
    "#time is the last dimension\n",
    "class ConvPositionalEncoding(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            channels, channels,\n",
    "            kernel_size=kernel_size,\n",
    "            groups=channels,\n",
    "            padding=kernel_size // 2,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)\n",
    "# channel is the last dimension\n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, dim, n_layers, n_heads, ffn_dim, dropout):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, \n",
    "        nhead=n_heads, \n",
    "        dim_feedforward=ffn_dim, \n",
    "        dropout=dropout, \n",
    "        activation='gelu', \n",
    "        norm_first=True, \n",
    "        batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "    \n",
    "class MaskedEncoderModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch,\n",
    "        dim,\n",
    "        mask_prob,\n",
    "        feature_enc_kernel_sizes,\n",
    "        feature_enc_strides,\n",
    "        feature_enc_paddings,\n",
    "        context_n_layers,\n",
    "        context_n_heads,\n",
    "        context_ffn_dim,\n",
    "        context_dropout=0.1,\n",
    "        use_cpe=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask_prob = mask_prob\n",
    "        self.feature_encoder = ConvFeatureEncoder(\n",
    "            in_ch=in_ch,\n",
    "            dim=dim,\n",
    "            kernel_sizes=feature_enc_kernel_sizes,\n",
    "            strides=feature_enc_strides,\n",
    "            paddings=feature_enc_paddings,\n",
    "        )\n",
    "        self.use_cpe = use_cpe\n",
    "        if use_cpe:\n",
    "            self.cpe = ConvPositionalEncoding(dim, kernel_size=3)\n",
    "\n",
    "        self.context_encoder = ContextEncoder(\n",
    "            dim=dim,\n",
    "            n_layers=context_n_layers,\n",
    "            n_heads=context_n_heads,\n",
    "            ffn_dim=context_ffn_dim,\n",
    "            dropout=context_dropout\n",
    "        )\n",
    "\n",
    "        self.mask_embedding = nn.Parameter(torch.zeros(dim))\n",
    "        nn.init.normal_(self.mask_embedding, mean=0.0, std=0.02)\n",
    "\n",
    "        self.pred_head = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def random_mask(x, mask_p):\n",
    "        B, T, D = x.shape\n",
    "        mask = torch.zeros(B, T, dtype=torch.bool, device=x.device)\n",
    "        for b in range(B):\n",
    "            where_to_mask = torch.bernoulli(torch.ones(T)*mask_p)\n",
    "            if(where_to_mask.sum() == 0):\n",
    "                mask[b, torch.rand(0,T)] = True\n",
    "            else:\n",
    "                mask[b] = where_to_mask == 1\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x, run_with_mask):\n",
    "        # 1) Conv feature encoder: (B, in_ch, T_raw) -> (B, dim, T_enc)\n",
    "        feats = self.feature_encoder(x)              # encoded targets\n",
    "\n",
    "        # 2) Optional convolutional positional encoding\n",
    "        if self.use_cpe:\n",
    "            feats = self.cpe(feats)                 # (B, dim, T_enc)\n",
    "\n",
    "\n",
    "        # 3) Prepare for transformer: (B, dim, T_enc) -> (B, T_enc, dim)\n",
    "        feats_t = feats.transpose(1, 2)             # original encoded features (targets)\n",
    "\n",
    "        if(run_with_mask):\n",
    "            # 4) Create masked input sequence\n",
    "            #masked_input = feats_t.clone()\n",
    "            #mask_bool = torch.zeros_like(feats_t[:, :, 0], dtype=torch.bool)  # (B, T_enc)\n",
    "            #mask_bool[torch.arange(0, feats_t.size(0)), torch.randint(0, feats_t.size(1), (feats_t.size(0),))] = True  # randomly mask 1 time step per sample\n",
    "            mask_bool = MaskedEncoderModel.random_mask(feats_t, self.mask_prob)\n",
    "            masked_input = feats_t.clone()\n",
    "            masked_input[mask_bool] = self.mask_embedding  # apply mask\n",
    "            ctx = self.context_encoder(masked_input)  # (B, T_enc, dim)\n",
    "            preds = self.pred_head(ctx)  # (B, T_enc, dim)\n",
    "            return preds, ctx, mask_bool\n",
    "        else:\n",
    "            ctx = self.context_encoder(feats_t)  # (B, T_enc, dim)\n",
    "            return ctx\n",
    "\n",
    "class LatentDirichletRegression(nn.Module):\n",
    "    def __init__(self,  input_dim, output_dim, kernel_sizes, channel_sizes, strides, paddings):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential()\n",
    "        for i, (k, s, p) in enumerate(zip(kernel_sizes, strides, paddings)):\n",
    "            conv = nn.Conv1d(\n",
    "                in_channels=input_dim if i == 0 else channel_sizes[i-1],\n",
    "                out_channels=channel_sizes[i],\n",
    "                kernel_size=k,\n",
    "                stride=s,\n",
    "                padding=p,\n",
    "            )\n",
    "            self.net.add_module(f\"conv_{i}\", conv)\n",
    "            self.net.add_module(f\"norm_{i}\", nn.GroupNorm(num_groups=channel_sizes[i], num_channels=channel_sizes[i]))\n",
    "            self.net.add_module(f\"gelu_{i}\", nn.GELU())\n",
    "\n",
    "        self.output_conv = nn.Sequential(nn.Conv1d(\n",
    "            in_channels=channel_sizes[-1],\n",
    "            out_channels=channel_sizes[-1],\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "        ),\n",
    "            nn.GELU())\n",
    "\n",
    "        self.output_alphas = nn.Linear(channel_sizes[-1], output_dim)\n",
    "        self.output_alpha0 = nn.Linear(channel_sizes[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = self.output_conv(x)\n",
    "        x = x.mean(dim=2)  # global average pooling over time dimension\n",
    "        alpha_scores = self.output_alphas(x)\n",
    "        alpha0 = F.softplus(self.output_alpha0(x))\n",
    "        return F.softmax(alpha_scores, dim=-1) * alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ee551",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'STEAD/'\n",
    "train_chunks = [ root + f'chunk{chunk}/chunk{chunk}.hdf5'  for chunk in range(2, 4) ]\n",
    "val_chunks = [ root + f'chunk{chunk}/chunk{chunk}.hdf5'  for chunk in range(4, 5) ]\n",
    "train_dataset = SteadDataset(train_chunks , channel_first=True)\n",
    "val_dataset = SteadDataset(val_chunks , channel_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "feature_enc_kernel_sizes=[10,8,4]\n",
    "feature_enc_strides=[5,4,2]\n",
    "feature_enc_paddings=[5,4,2]\n",
    "p_mask = 0.15\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MaskedEncoderModel(\n",
    "    in_ch=input_dim,\n",
    "    dim=256,\n",
    "    mask_prob=p_mask,\n",
    "    feature_enc_kernel_sizes=feature_enc_kernel_sizes,\n",
    "    feature_enc_strides=feature_enc_strides,\n",
    "    feature_enc_paddings=feature_enc_paddings,\n",
    "    context_n_layers=6,\n",
    "    context_n_heads=4,\n",
    "    context_ffn_dim=1024,\n",
    "    context_dropout=0.1,\n",
    "    use_cpe=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2859f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('STEAD/maskedencoder_epoch10.pth', map_location=device ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac4078",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0) # must be 0 for hdf5\n",
    "steps_per_epoch = len(train_loader)\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "learning_rate = 3e-4\n",
    "warmup_steps = int(0.1 * num_epochs * steps_per_epoch)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[\n",
    "        LinearLR(optimizer, start_factor=1e-3, total_iters=warmup_steps),\n",
    "        CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=num_epochs * steps_per_epoch - warmup_steps,\n",
    "            eta_min=1e-6\n",
    "        ),\n",
    "    ],\n",
    "    milestones=[warmup_steps]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14863632",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_var = 1e-3\n",
    "def vicreg_reg(z, mask, eps=1e-4, gamma=1.0):\n",
    "    # z: [B,T,C], mask: [B,T] True=masked\n",
    "    u = z[~mask]                           # [N,C]\n",
    "    if u.shape[0] < 2:\n",
    "        return z.sum() * 0.0\n",
    "\n",
    "    u = u - u.mean(dim=0, keepdim=True)\n",
    "\n",
    "    std = torch.sqrt(u.var(dim=0, unbiased=False) + eps)\n",
    "    var_loss = 0.5*torch.mean((std-gamma)**2)\n",
    "\n",
    "    # covariance term (decorrelate dims)\n",
    "    N, C = u.shape\n",
    "    cov = (u.T @ u) / (N - 1)              # [C,C]\n",
    "    offdiag = cov - torch.diag(torch.diag(cov))\n",
    "    cov_loss = (offdiag**2).mean()\n",
    "\n",
    "    return var_loss + 0.01 * cov_loss, std      # 0.01 is a decent start\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (traces, p_arrivals, s_arrivals, coda_ends, event_names) in enumerate(train_loader):\n",
    "        traces_mean = traces.mean(dim=2, keepdim=True)\n",
    "        traces_std = traces.std(dim=2, keepdim=True) + 1e-9\n",
    "        normalized_traces = (traces - traces_mean) / traces_std  # normalize input traces\n",
    "        optimizer.zero_grad()\n",
    "        ctx_preds, ctx, mask_bool = model(normalized_traces.to(device), run_with_mask=True)\n",
    "        masked_preds = ctx_preds[mask_bool]\n",
    "        masked_targets = ctx.detach()[mask_bool]\n",
    "        loss = F.mse_loss(masked_preds, masked_targets) \n",
    "        reg_loss, unmasked_ctx_std = vicreg_reg(ctx, mask_bool)\n",
    "        loss += lambda_var * reg_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if(i % 250 == 0):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.1e}\")\n",
    "            with torch.no_grad():\n",
    "                print(unmasked_ctx_std.cpu().mean().item())\n",
    "\n",
    "    torch.save(model.state_dict(), f'STEAD/maskedencoder_epoch{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5cd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False, num_workers=0) # must be 0 for hdf5\n",
    "with torch.no_grad():\n",
    "    for i, (traces, p_arrivals, s_arrivals, coda_ends, event_names) in enumerate(val_loader):\n",
    "        traces_mean = traces.mean(dim=2, keepdim=True)\n",
    "        traces_std = traces.std(dim=2, keepdim=True) + 1e-9\n",
    "        normalized_traces = (traces - traces_mean) / traces_std\n",
    "        ctx = model(normalized_traces.to(device), run_with_mask=False)\n",
    "        print(ctx)\n",
    "        print(ctx.var(dim=(0,1)).mean())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46efabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ctx = model(normalized_traces.to(device), run_with_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(ctx.size(0)):\n",
    "    plt.plot(ctx[idx].cpu().numpy().mean(axis=0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efdd028",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "interval_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0) # must be 0 for hdf5\n",
    "interval_model = LatentDirichletRegression(\n",
    "    input_dim=256,\n",
    "    output_dim=4,\n",
    "    kernel_sizes=[3,3,3],\n",
    "    channel_sizes=[128,64,32],\n",
    "    strides=[2,2,2],\n",
    "    paddings=[1,1,1],).to(device)\n",
    "optimizer_interval = optim.Adam(interval_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "steps_per_epoch = len(interval_dataloader)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer_interval,\n",
    "    T_0=2 * steps_per_epoch,\n",
    "    T_mult=2,\n",
    "    eta_min=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_model.load_state_dict(torch.load('STEAD/latdirichlet_epoch10.pth', map_location=device ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e83103",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (traces, p_arrivals, s_arrivals, coda_ends, event_names) in enumerate(interval_dataloader):\n",
    "        traces_mean = traces.mean(dim=2, keepdim=True)\n",
    "        traces_std = traces.std(dim=2, keepdim=True) + 1e-9\n",
    "        normalized_traces = (traces - traces_mean) / traces_std  # normalize input traces\n",
    "        num_timesteps = traces.size(-1)\n",
    "        with torch.no_grad():\n",
    "            ctx = model(normalized_traces.to(device), run_with_mask=False)  # (B, T_enc, dim)\n",
    "            ctx_t = ctx.transpose(1, 2)  # (B, dim, T_enc)\n",
    "        optimizer_interval.zero_grad()\n",
    "        alphas = interval_model(ctx_t)  # (B, T_out)\n",
    "        dist = torch.distributions.Dirichlet(alphas + 1e-9)\n",
    "        s1 = p_arrivals/num_timesteps\n",
    "        s2 = s_arrivals/num_timesteps - s1\n",
    "        s3 = coda_ends/num_timesteps - s1 - s2\n",
    "        s4 = 1.0 - (s1 + s2 + s3)\n",
    "        target = torch.stack([s1, s2, s3, s4], dim=-1)\n",
    "        loss = -dist.log_prob(target.to(device)).mean()\n",
    "        loss.backward()\n",
    "        optimizer_interval.step()\n",
    "        scheduler.step()\n",
    "        if(i % 100 == 0):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(interval_dataloader)}], Loss: {loss.item():.1e}\")\n",
    "    torch.save(interval_model.state_dict(), f'STEAD/latdirichlet_epoch{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b948c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    trues = []\n",
    "    preds = []\n",
    "    pred_alphas = []\n",
    "    ntraces = []\n",
    "    for i, (traces, p_arrivals, s_arrivals, coda_ends, event_names) in enumerate(interval_dataloader):\n",
    "        traces_mean = traces.mean(dim=2, keepdim=True)\n",
    "        traces_std = traces.std(dim=2, keepdim=True) + 1e-9\n",
    "        normalized_traces = (traces - traces_mean) / traces_std  # normalize input traces\n",
    "        num_timesteps = traces.size(-1)\n",
    "        ctx = model(normalized_traces.to(device), run_with_mask=False)  # (B, T_enc, dim)\n",
    "        ctx_t = ctx.transpose(1, 2)  # (B, dim, T_enc)\n",
    "        alphas = interval_model(ctx_t)  # (B, T_out)\n",
    "        dist = torch.distributions.Dirichlet(alphas + 1e-9)\n",
    "        s1 = p_arrivals/num_timesteps\n",
    "        s2 = s_arrivals/num_timesteps - s1\n",
    "        s3 = coda_ends/num_timesteps - s1 - s2\n",
    "        s4 = 1.0 - (s1 + s2 + s3)\n",
    "        target = torch.stack([s1, s2, s3, s4], dim=-1)\n",
    "        trues.append(target.cpu().numpy())\n",
    "        preds.append(dist.mean.cpu().numpy())\n",
    "        ntraces.append(normalized_traces)\n",
    "        pred_alphas.append(alphas.cpu().numpy())\n",
    "        if(i>=5):\n",
    "            break\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    ntraces = np.concatenate(ntraces, axis=0)\n",
    "    pred_alphas = np.concatenate(pred_alphas, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "for i in range(trues.shape[0]):\n",
    "    #print(stats.linregress(trues[:, i], preds[:, i]))\n",
    "    true_times = np.cumsum(trues[i])\n",
    "    pred_times = np.cumsum(preds[i])\n",
    "    distr = torch.distributions.Dirichlet(torch.from_numpy(pred_alphas[i]))\n",
    "    pred_samples = np.cumsum(distr.sample((10000,)).numpy(), axis=-1)\n",
    "\n",
    "    plt.plot(np.linspace(0,1, ntraces.shape[-1]), ntraces[i,0,:], color='gray')\n",
    "    y_min, y_max = plt.gca().get_ylim()\n",
    "    plt.vlines(true_times[:-1], y_min, y_max, color='red')\n",
    "    ax2 = plt.gca().twinx()\n",
    "    ax2.hist(pred_samples[:,0], bins=100, density=True, alpha=0.5)\n",
    "    ax2.hist(pred_samples[:,1], bins=100, density=True, alpha=0.5)\n",
    "    ax2.hist(pred_samples[:,2], bins=100, density=True, alpha=0.5)\n",
    "    plt.show()\n",
    "    if(i>=20):\n",
    "        break\n",
    "    #plt.vlines(pred_times[:-1], y_min, y_max, color='red')\n",
    "    #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
